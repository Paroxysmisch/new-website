---
title: Yash Shah
summary: Computer Science student at the University of Cambridge
math: typst
sidebar_title: Home
external_links:
  "LinkedIn": https://www.linkedin.com/in/yash-shah-100/
  "GitHub": https://github.com/Paroxysmisch
  "Personal email": mailto:yashshah00@outlook.com
  "Cambridge email": mailto:ys562@cam.ac.uk
  "CV": https://typst.app/project/rpzUzUgdcZgP1tP1b0BRCB
---

![image](https://images.contentstack.io/v3/assets/bltcedd8dbd5891265b/blt4a4af7e6facea579/6668df6ceca9a600983250ac/beautiful-flowers-hero.jpg?q=70&width=3840&auto=webp)

## About me
I am a fourth year computer science student interested in applying Geometric Deep Learning (mostly Graph Neural Networks) and Neural Algorithmic Reasoning to real-world problems. My interests include accelerating Computational Genomics, Computer Architecture, Natural Language Processing, and Graphics with machine learning. I am currently working on the use of GNNs and NAR for accelerating de novo genome assembly.

## Academic interests
/// details | Geometric Deep Learning
(GDL) is a framework leveraging the geometry in data, through groups, representations, and principles of invariance and equivariance, to learn more effective machine learning models. 

Central to GDL are symmetriesâ€”transformations that leave an object unchanged. In the context of machine learning, relevant symmetries can arise in various forms: symmetries of the input data (e.g. rotational symmetries in molecular structure); the label function mapping the input to some output (e.g. the image classification function is invariant to the location of the object in the image), the domain our data lives on (e.g. data living on a set is invariant to the permutation of items in the set), or even symmetries in the model's parameterization.

Through encoding symmetry within our model architecture, we restrict the space of functions that can be represented to those that respect these symmetries. This makes models more performant, improves generalization, and can make learning more sample/data efficient.

I am interested in utilizing the GDL framework to improve model architectures for existing tasks, as well as designing new approaches for tackling emerging problem domains.
///

/// details | Natural Language Processing
(NLP) has been a key interest of mine, where my focus lies in the study and improvement of the Transformer architecture. I have worked on improving the positional embeddings, alternative attention mechanisms, and exploring use with other architectures such as the Mamba Selective State Space model, for instance. Additionally, I have looked exploring these architectures from a theoretical standpoint using the tools provided by GDL to draw novel insights.
///

/// details | Reinforcement Learning
(RL) is a paradigm for learning optimal behavior through interaction with an environment. My interest in RL stems from its potential to aid with scientific discovery, experimenting with novel model architectures such as the Mamba Selective State Space model, and Graph Neural Networks.
///

/// details | Computer Architecture
My interest in computer architecture lies in the interface between machine learning models and the hardware they run on, as well as improving and designing new architectures for machine learning workloads.
///

